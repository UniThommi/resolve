{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Neural Processes (CNP).\n",
    "[Conditional Neural Processes](https://arxiv.org/pdf/1807.01613.pdf) (CNPs) were\n",
    "introduced as a continuation of\n",
    "[Generative Query Networks](https://deepmind.com/blog/neural-scene-representation-and-rendering/)\n",
    "(GQN) to extend its training regime to tasks beyond scene rendering, e.g. to\n",
    "regression and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from resolve.utilities import plotting_utils_cnp as plotting\n",
    "from resolve.utilities import utilities as utils\n",
    "from resolve.conditional_neural_process import DataGeneration\n",
    "from resolve.conditional_neural_process import DeterministicModel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import yaml\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the yaml settings file here\n",
    "path_to_settings = \"/global/cfs/projectdirs/legend/users/tbuerger/neuralNet/binaryBH/resolve/examples/binary-black-hole/\"\n",
    "with open(f\"{path_to_settings}/settings.yaml\", \"r\") as f:\n",
    "    config_file = yaml.safe_load(f)\n",
    "\n",
    "TRAINING_EPOCHS = int(config_file[\"cnp_settings\"][\"training_epochs\"]) # Total number of training points: training_iterations * batch_size * max_content_points\n",
    "torch.manual_seed(0)\n",
    "BATCH_SIZE = config_file[\"cnp_settings\"][\"batch_size_train\"]\n",
    "LEARNING_RATE = 0.00001\n",
    "FILES_PER_BATCH = config_file[\"cnp_settings\"][\"files_per_batch_train\"]\n",
    "CONTEXT_IS_SUBSET = config_file[\"cnp_settings\"][\"context_is_subset\"]\n",
    "CONTEXT_RATIO = config_file[\"cnp_settings\"][\"context_ratio\"]\n",
    "\n",
    "# Other Parameters\n",
    "TEST_AFTER = int(config_file[\"cnp_settings\"][\"test_after\"])\n",
    "torch.manual_seed(0)\n",
    "target_range = config_file[\"simulation_settings\"][\"target_range\"]\n",
    "is_binary = target_range[0] >= 0 and target_range[1] <= 1\n",
    "version = config_file[\"path_settings\"][\"version\"]\n",
    "path_out = f'{config_file[\"path_settings\"][\"path_out_cnp\"]}/{version}'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size, y_size = utils.get_feature_and_label_size(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Augmentation in Progress:   2%|▏         | 20/1000 [00:00<00:21, 46.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Augmentation in Progress: 100%|██████████| 1000/1000 [00:22<00:00, 45.36it/s]\n",
      "Training Epoch 1/1: 9948it [22:18, 10.48it/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished full dataset pass. Starting new epoch! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/1: 9950it [22:19,  7.43it/s]\n"
     ]
    }
   ],
   "source": [
    "d_x, d_in, representation_size, d_out = x_size , x_size+y_size, config_file[\"cnp_settings\"][\"representation_size\"], y_size*2\n",
    "encoder_sizes = [d_in] + config_file[\"cnp_settings\"][\"encoder_hidden_layers\"] + [representation_size]\n",
    "decoder_sizes = [representation_size + d_x]+ config_file[\"cnp_settings\"][\"decoder_hidden_layers\"] + [d_out] \n",
    "\n",
    "model = DeterministicModel(encoder_sizes, decoder_sizes)\n",
    "os.system(f'mkdir -p {path_out}/cnp_{version}_tensorboard_logs/arxiv')\n",
    "os.system(f'mv {path_out}/cnp_{version}_tensorboard_logs/events* {path_out}/cnp_{version}_tensorboard_logs/arxiv/')\n",
    "writer = SummaryWriter(log_dir=f'{path_out}/cnp_{version}_tensorboard_logs')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "\n",
    "USE_DATA_AUGMENTATION = config_file[\"cnp_settings\"][\"use_data_augmentation\"]\n",
    "# load data:\n",
    "dataset_train = DataGeneration(mode = \"training\", \n",
    "                                config_file=config_file, \n",
    "                                path_to_files=config_file[\"path_settings\"][\"path_to_files_train\"], \n",
    "                                use_data_augmentation=USE_DATA_AUGMENTATION, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                files_per_batch=FILES_PER_BATCH,\n",
    "                                context_ratio=CONTEXT_RATIO)\n",
    "dataset_train.set_loader()\n",
    "dataloader_train = dataset_train.dataloader\n",
    "\n",
    "dataset_test = DataGeneration(mode = \"training\", \n",
    "                                config_file=config_file, \n",
    "                                path_to_files=config_file[\"path_settings\"][\"path_to_files_testing\"], \n",
    "                                use_data_augmentation=False, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                files_per_batch=FILES_PER_BATCH,\n",
    "                                context_ratio=CONTEXT_RATIO)\n",
    "dataset_test.set_loader()\n",
    "dataloader_test = dataset_test.dataloader\n",
    "\n",
    "\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "for it_epoch in range(TRAINING_EPOCHS):\n",
    "    data_iter = iter(dataloader_test)\n",
    "    for b, batch in tqdm(enumerate(dataloader_train), total=math.ceil(len(dataloader_train)/BATCH_SIZE), desc=\"Training Epoch {}/{}\".format(it_epoch+1, TRAINING_EPOCHS)):\n",
    "        it_step = it_epoch * len(dataloader_train) + b\n",
    "        batch_formated=dataset_train.format_batch_for_cnp(batch, CONTEXT_IS_SUBSET)\n",
    "        # Get the predicted mean and variance at the target points for the testing set\n",
    "        log_prob, mu, _ = model(batch_formated.query, batch_formated.target_y, is_binary)\n",
    "        \n",
    "        # Define the loss\n",
    "        loss = -log_prob.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient descent to update parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "        # reset gradient to 0 on all parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if is_binary:\n",
    "            loss_bce = bce(mu, batch_formated.target_y)\n",
    "        else:\n",
    "            loss_bce=-1\n",
    "        \n",
    "        # Inside your batch loop, right after computing losses:\n",
    "        writer.add_scalar('Loss/Logprob/train', loss.item(), it_step)\n",
    "        y_pred = mu[0].detach().cpu().numpy().flatten()\n",
    "        y_true = batch_formated.target_y[0].detach().cpu().numpy().flatten()\n",
    "        mae = mean_absolute_error(y_true,y_pred)\n",
    "        mse = mean_squared_error(y_true,y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        writer.add_scalar('Metric/Mae/Test', mae, it_step)\n",
    "        writer.add_scalar('Metric/Mse/Test', mse, it_step)\n",
    "        writer.add_scalar('Metric/R2/Test', r2, it_step)\n",
    "        if is_binary:\n",
    "            writer.add_scalar('Loss/BCE/train', loss_bce.item(), it_step)\n",
    "            y_pred = (y_pred > 0.5).astype(int)\n",
    "            y_true = y_true.astype(int)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            writer.add_scalar('Accuracy/train', acc, global_step=it_step)\n",
    "            from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            writer.add_scalar('Metrics/Precision/Train', precision, global_step=it_step)\n",
    "            writer.add_scalar('Metrics/Recall/Train', recall, global_step=it_step)\n",
    "            writer.add_scalar('Metrics/F1/Train', f1, global_step=it_step)\n",
    "        \n",
    "        mu=mu[0].detach().numpy()\n",
    "        if b % TEST_AFTER == 0:\n",
    "            try:\n",
    "                batch_testing = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(dataloader_test)\n",
    "                batch_testing = next(data_iter)\n",
    "\n",
    "            batch_formated_test=dataset_test.format_batch_for_cnp(batch_testing, CONTEXT_IS_SUBSET)\n",
    "          \n",
    "            log_prob_testing, mu_testing, _ = model(batch_formated_test.query, batch_formated_test.target_y, is_binary)\n",
    "            loss_testing = -log_prob_testing.mean()\n",
    "            \n",
    "            if is_binary:\n",
    "                loss_bce_testing = bce(mu_testing,  batch_formated_test.target_y)\n",
    "            else:\n",
    "                loss_bce_testing = -1.\n",
    "\n",
    "            writer.add_scalar('Loss/Logprob/Test', loss_testing.item(), b+it_epoch * len(dataloader_train))\n",
    "            if is_binary:\n",
    "                writer.add_scalar('Loss/BCE/Test', loss_bce_testing.item(),b+it_epoch * len(dataloader_train))\n",
    "                y_pred = (mu_testing[0].detach().cpu().numpy() > 0.5).astype(int).flatten()\n",
    "                y_true = batch_formated_test.target_y[0].detach().cpu().numpy().astype(int).flatten()\n",
    "                cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "                TN, FP, FN, TP = cm.ravel()\n",
    "                fig, ax = plt.subplots()\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "                ax.set_xlabel('Predicted Label')\n",
    "                ax.set_ylabel('True Label')\n",
    "                ax.set_title(f'Confusion Matrix at step {it_step}')\n",
    "                writer.add_figure('ConfusionMatrix', fig, global_step=it_step)\n",
    "                writer.add_scalar('ConfusionMatrix/FalsePositives/Test', cm[0, 1], it_step)\n",
    "                writer.add_scalar('ConfusionMatrix/FalseNegatives/Test', cm[1, 0], it_step)\n",
    "                # Predicted probabilities\n",
    "                acc = accuracy_score(y_true, y_pred)\n",
    "                writer.add_scalar('Accuracy/Test', acc, global_step=it_step)\n",
    "                precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "                recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "                f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "                writer.add_scalar('Metrics/Precision/Train', precision, global_step=it_step)\n",
    "                writer.add_scalar('Metrics/Recall/Train', recall, global_step=it_step)\n",
    "                writer.add_scalar('Metrics/F1/Train', f1, global_step=it_step)\n",
    "\n",
    "            mu_testing = mu_testing[0].detach().numpy()\n",
    "\n",
    "            y_true = batch_formated_test.target_y[0].detach().numpy()\n",
    "            mae = mean_absolute_error(y_true,mu_testing)\n",
    "            mse = mean_squared_error(y_true,mu_testing)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            writer.add_scalar('Metric/Mae/Test', mae, it_step)\n",
    "            writer.add_scalar('Metric/Mse/Test', mse, it_step)\n",
    "            writer.add_scalar('Metric/R2/Test', r2, it_step)\n",
    "\n",
    "            if y_size ==1:\n",
    "                fig = plotting.plot(mu, batch_formated.target_y[0].detach().numpy(), f'{loss:.2f}', mu_testing, batch_formated_test.target_y[0].detach().numpy(), f'{loss_testing:.2f}', target_range, it_step)\n",
    "                writer.add_figure('Prediction/train_vs_test', fig, global_step=it_step)\n",
    "            else:\n",
    "                for k in range(y_size):\n",
    "                    fig = plotting.plot(mu[:,k], batch_formated.target_y[0].detach().numpy()[:,k], f'{loss:.2f}', mu_testing[:,k], batch_formated_test.target_y[0].detach().numpy()[:,k], f'{loss_testing:.2f}', target_range, it_step)\n",
    "                    writer.add_figure(f'Prediction/train_vs_test_k{k}', fig, global_step=it_step)\n",
    "\n",
    "\n",
    "\n",
    "writer.close()\n",
    "torch.save(model.state_dict(), f'{path_out}/cnp_{version}_model.pth')\n",
    "\n",
    "config_file[\"feature_settings\"][\"x_mean\"] = dataset_train.feature_mean.numpy().tolist()\n",
    "config_file[\"feature_settings\"][\"x_std\"] = dataset_train.feature_std.numpy().tolist()\n",
    "# Save back to the file\n",
    "with open(f\"{path_to_settings}/settings.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(config_file, f, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Summary written to:\n",
      "  - /global/cfs/projectdirs/legend/users/tbuerger/neuralNet/binaryBH/resolve/examples/binary-black-hole/out/cnp/v1.3/cnp_v1.3_model.pth\n",
      "  - /global/cfs/projectdirs/legend/users/tbuerger/neuralNet/binaryBH/resolve/examples/binary-black-hole/out/cnp/v1.3/cnp_v1.3_tensorboard_logs/\n",
      "  - /global/cfs/projectdirs/legend/users/tbuerger/neuralNet/binaryBH/resolve/examples/binary-black-hole/out/cnp/v1.3/cnp_v1.3_summary.json\n"
     ]
    }
   ],
   "source": [
    "# === Summarize final metrics and hyperparameters ===\n",
    "\n",
    "# Extract or define final metrics manually if not tracked\n",
    "final_summary = {\n",
    "    \"Final Hyperparameters\": {\n",
    "        \"Encoder Sizes\": encoder_sizes,\n",
    "        \"Decoder Sizes\": decoder_sizes,\n",
    "        \"Representation Size\": representation_size,\n",
    "        \"Learning Rate\": LEARNING_RATE,\n",
    "        \"Batch Size\": BATCH_SIZE,\n",
    "        \"Files per Batch\": FILES_PER_BATCH,\n",
    "        \"Training Epochs\": TRAINING_EPOCHS,\n",
    "        \"Binary Task\": is_binary,\n",
    "        \"Data Augmentation\": config_file[\"cnp_settings\"][\"use_data_augmentation\"],\n",
    "        \"Context is Subset\": config_file[\"cnp_settings\"][\"context_is_subset\"],\n",
    "    },\n",
    "    \"Output Path\": path_out,\n",
    "    \"Model Checkpoint\": f'{path_out}/cnp_{version}_model.pth',\n",
    "    \"TensorBoard Logs\": f'{path_out}/cnp_{version}_tensorboard_logs',\n",
    "}\n",
    "\n",
    "# Save to readable JSON summary\n",
    "import json\n",
    "with open(f\"{path_out}/cnp_{version}_summary.json\", \"w\") as f:\n",
    "    json.dump(final_summary, f, indent=5)\n",
    "\n",
    "print(\"Training complete. Summary written to:\")\n",
    "print(f\"  - {path_out}/cnp_{version}_model.pth\")\n",
    "print(f\"  - {path_out}/cnp_{version}_tensorboard_logs/\")\n",
    "print(f\"  - {path_out}/cnp_{version}_summary.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Final Train LogProb Loss\": 0.6118854284286499,\n",
      "    \"Final Train BCE Loss\": 0.3491189777851105,\n",
      "    \"Final Test LogProb Loss\": 0.7859804630279541,\n",
      "    \"Final Test BCE Loss\": 0.6482114195823669,\n",
      "    \"Final Accuracy Test\": 0.52046783625731,\n",
      "    \"Final Precision\": 0.15104166666666666,\n",
      "    \"Final Recall\": 0.9666666666666667,\n",
      "    \"Final F1\": 0.26126126126126126,\n",
      "    \"Final MAE\": 0.16311222314834595,\n",
      "    \"Final MSE\": 0.0570099800825119,\n",
      "    \"Final R2\": 0.7206395268440247\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = {\n",
    "    \"Final Train LogProb Loss\": loss.item(),\n",
    "    \"Final Train BCE Loss\": loss_bce.item() if is_binary else None,\n",
    "    \"Final Test LogProb Loss\": loss_testing.item(),\n",
    "    \"Final Test BCE Loss\": loss_bce_testing.item() if is_binary else None,\n",
    "    \"Final Accuracy Test\": acc,\n",
    "    \"Final Precision\": precision,\n",
    "    \"Final Recall\": recall,\n",
    "    \"Final F1\": f1,\n",
    "    \"Final MAE\": mae,\n",
    "    \"Final MSE\": mse,\n",
    "    \"Final R2\": r2,\n",
    "}\n",
    "\n",
    "# Add to summary\n",
    "final_summary[\"Final Metrics\"] = final_metrics\n",
    "\n",
    "print(json.dumps(final_summary[\"Final Metrics\"], indent=4))\n",
    "# Update JSON\n",
    "with open(f\"{path_out}/cnp_{version}_summary.json\", \"w\") as f:\n",
    "    json.dump(final_summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary to csv\n",
    "# Prepare the final row dictionary\n",
    "row = {\n",
    "    # Hyperparameters\n",
    "    \"Encoder Sizes\": encoder_sizes,\n",
    "    \"Decoder Sizes\": decoder_sizes,\n",
    "    \"Representation Size\": representation_size,\n",
    "    \"Learning Rate\": LEARNING_RATE,\n",
    "    \"Batch Size\": BATCH_SIZE,\n",
    "    \"Files per Batch\": FILES_PER_BATCH,\n",
    "    \"Training Epochs\": TRAINING_EPOCHS,\n",
    "    \"Context Ratio\": CONTEXT_RATIO,\n",
    "    \"Binary Task\": is_binary,\n",
    "    \"Data Augmentation\": config_file[\"cnp_settings\"][\"use_data_augmentation\"],\n",
    "    \"Context is Subset\": CONTEXT_IS_SUBSET,\n",
    "    \"Output Path\": path_out,\n",
    "    \"Model Checkpoint\": f'{path_out}/cnp_{version}_model.pth',\n",
    "    \"TensorBoard Logs\": f'{path_out}/cnp_{version}_tensorboard_logs',\n",
    "\n",
    "    # Final Metrics\n",
    "    \"Final Train LogProb Loss\": loss.item(),\n",
    "    \"Final Train BCE Loss\": loss_bce.item() if is_binary else None,\n",
    "    \"Final Test LogProb Loss\": loss_testing.item(),\n",
    "    \"Final Test BCE Loss\": loss_bce_testing.item() if is_binary else None,\n",
    "    \"Final Accuracy Test\": acc,\n",
    "    \"Final Precision\": precision,\n",
    "    \"Final Recall\": recall,\n",
    "    \"Final F1\": f1,\n",
    "    \"Final MAE\": mae,\n",
    "    \"Final MSE\": mse,\n",
    "    \"Final R2\": r2,\n",
    "\n",
    "    # Confusion Matrix Elements (only meaningful for binary task)\n",
    "    \"True Positives\": TP if is_binary else None,\n",
    "    \"False Positives\": FP if is_binary else None,\n",
    "    \"True Negatives\": TN if is_binary else None,\n",
    "    \"False Negatives\": FN if is_binary else None,\n",
    "}\n",
    "\n",
    "csv_path = f\"{path_out}/cnp_training_summary.csv\"\n",
    "file_exists = os.path.isfile(csv_path)\n",
    "\n",
    "with open(csv_path, mode='a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=row.keys())\n",
    "\n",
    "    if not file_exists:\n",
    "        writer.writeheader()\n",
    "    writer.writerow(row)\n",
    "\n",
    "# print(f\"Run {runs} in Summary csv geschrieben\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Start TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in terminal:\n",
    "\n",
    "tensorboard --logdir=\\<path to tensor board log dir\\> --host=0.0.0.0 --port=7007\n",
    "\n",
    "Open:\n",
    "\n",
    "http://localhost:7007/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
